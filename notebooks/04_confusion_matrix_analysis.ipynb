# Confusion Matrix Analysis Notebook

This notebook analyzes confusion matrices from model experiments.

## Setup

```python
import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd().parent))

from src.evaluation.confusion_matrix import ConfusionMatrixGenerator
from src.config import config
import json
import matplotlib.pyplot as plt
```

## Load Results

```python
# Load evaluation results
results_path = config.RETRIEVAL_RESULTS_DIR / "evaluation_results.json"

if results_path.exists():
    with open(results_path, 'r') as f:
        results = json.load(f)
    
    print("Models evaluated:")
    for model_name in results.keys():
        print(f"  - {model_name}")
else:
    print("No results found. Run experiments first.")
```

## Analyze Confusion Matrices

```python
cm_generator = ConfusionMatrixGenerator()

# For each model
for model_name, model_results in results.items():
    print(f"\n{model_name}:")
    
    # Get confusion matrix
    cm = model_results[
    "confusion_matrix"
]
    
    # Calculate per-class metrics
    metrics = cm_generator.calculate_per_class_metrics(cm)
    
    for category, cat_metrics in metrics.items():
        print(f"  {category}:")
        print(f"    Precision: {cat_metrics['precision']:.4f}")
        print(f"    Recall: {cat_metrics['recall']:.4f}")
        print(f"    F1: {cat_metrics['f1']:.4f}")
```

## Compare Models

```python
# Extract metrics for comparison
model_names = list(results.keys())
accuracies = [results[m
    ][
        "metrics"
    ][
        "accuracy"
    ] for m in model_names
]
maps = [results[m
    ][
        "metrics"
    ][
        "map"
    ] for m in model_names
]

# Plot comparison
fig, axes = plt.subplots(1,
2, figsize=(12,
5))

axes[
    0
].bar(model_names, accuracies)
axes[
    0
].set_title("Accuracy Comparison")
axes[
    0
].set_ylabel("Accuracy")
axes[
    0
].set_ylim([
    0,
    1
])

axes[
    1
].bar(model_names, maps)
axes[
    1
].set_title("MAP Comparison")
axes[
    1
].set_ylabel("MAP")
axes[
    1
].set_ylim([
    0,
    1
])

plt.tight_layout()
plt.savefig(config.PLOTS_DIR / "model_comparison.png", dpi=300)
plt.show()
```

## View Confusion Matrix Images

```python
from IPython.display import Image, display

# Display confusion matrices
for model_name in results.keys():
    cm_path = config.CONFUSION_MATRICES_DIR / f"{model_name}_confusion_matrix.png"
    if cm_path.exists():
        print(f"\n{model_name} Confusion Matrix:")
        display(Image(filename=str(cm_path)))
```
